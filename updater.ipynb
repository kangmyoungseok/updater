{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "updater.ipynb",
      "provenance": [],
      "mount_file_id": "1WFzrTb6rZGCx3dZf8ETv4OyghM5YVKwC",
      "authorship_tag": "ABX9TyMOSvINAuB0iX7jP18xXUU5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kangmyoungseok/updater/blob/main/updater.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN3HDbUJVbjL"
      },
      "source": [
        "# 1. pair.py\n",
        "\n",
        "\n",
        "실행 -> pairs_v1.x.csv파일 나옴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV0uwzR71E28"
      },
      "source": [
        "from pprint import pprint\n",
        "from requests import Request, Session\n",
        "from requests.exceptions import ConnectionError, Timeout, TooManyRedirects\n",
        "import pandas as pd\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import re # 추가\n",
        "from urllib.request import urlopen\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# swap function makes scam token to token0\n",
        "def switch_token(result):\n",
        "    for pair in result['data']['pairs']:\n",
        "        if (int(pair['token0']['txCount']) > int(pair['token1']['txCount'] )):\n",
        "            pair['reserve00'],pair['reserve11'] = pair['reserve1'],pair['reserve0']\n",
        "            pair['token00'],pair['token11'] = pair['token1'],pair['token0']\n",
        "        else:\n",
        "            pair['reserve00'],pair['reserve11'] = pair['reserve0'],pair['reserve1']\n",
        "            pair['token00'],pair['token11'] = pair['token0'],pair['token1']\n",
        "    \n",
        "# function to use requests.post to make an API call to the subgraph url\n",
        "def run_query(query):\n",
        "\n",
        "    # endpoint where you are making the request\n",
        "    request = requests.post('https://api.thegraph.com/subgraphs/name/uniswap/uniswap-v2'\n",
        "                            '',\n",
        "                            json={'query': query})\n",
        "    if request.status_code == 200:\n",
        "        return request.json()\n",
        "    else:\n",
        "        raise Exception('Query failed. return code is {}.      {}'.format(request.status_code, query))\n",
        "\n",
        "\n",
        "query_init = '''\n",
        "{\n",
        " pairs(first: 1000, orderBy: createdAtBlockNumber, orderDirection: desc) {\n",
        "   id\n",
        "   token0{\n",
        "    id\n",
        "    symbol\n",
        "    name\n",
        "    txCount\n",
        "    totalLiquidity\n",
        "  }\n",
        "   token1{\n",
        "    id\n",
        "    symbol\n",
        "    name\n",
        "    txCount\n",
        "    totalLiquidity\n",
        "  }\n",
        "   reserve0\n",
        "   reserve1\n",
        "   totalSupply\n",
        "   reserveUSD\n",
        "   reserveETH\n",
        "   txCount\n",
        "   createdAtTimestamp\n",
        "   createdAtBlockNumber\n",
        " }\n",
        "}\n",
        "''' \n",
        "\n",
        "\n",
        "query_iter = '''\n",
        "{\n",
        " pairs(first: 1000, orderBy: createdAtBlockNumber, orderDirection: desc, where: {createdAtBlockNumber_lt:initial}) {\n",
        "   id\n",
        "   token0{\n",
        "    id\n",
        "    symbol\n",
        "    name\n",
        "    txCount\n",
        "    totalLiquidity\n",
        "  }\n",
        "   token1{\n",
        "    id\n",
        "    symbol\n",
        "    name\n",
        "    txCount\n",
        "    totalLiquidity\n",
        "  }\n",
        "   reserve0\n",
        "   reserve1\n",
        "   totalSupply\n",
        "   reserveUSD\n",
        "   reserveETH\n",
        "   txCount\n",
        "   createdAtTimestamp\n",
        "   createdAtBlockNumber\n",
        " }\n",
        "}\n",
        "''' \n",
        "\n",
        "pair_frame = [] # 쿼리의 결과를 여기에 List 형태로 담을 것. 50000개\n",
        "\n",
        "##### 맨 처음 쿼리. 반복문 불가####\n",
        "query = query_init\n",
        "result = run_query(query)\n",
        "switch_token(result)\n",
        "for pair in result['data']['pairs']:\n",
        "    if((pair['token0']['symbol'] != 'WETH') and (pair['token1']['symbol'] !='WETH' )):\n",
        "      continue\n",
        "    if((pair['token00']['txCount'] == 0) or (pair['token00']['txCount'] == '0')):\n",
        "      continue\n",
        "    year = time.gmtime(int(pair['createdAtTimestamp'])).tm_year\n",
        "    month = time.gmtime(int(pair['createdAtTimestamp'])).tm_mon\n",
        "    day = time.gmtime(int(pair['createdAtTimestamp'])).tm_mday\n",
        "    pair['createdAtTimestamp'] = str(year) + '-' + str(month) + '-' + str(day)\n",
        "    pair_frame.append(pair)\n",
        "\n",
        "last_block = result['data']['pairs'][999]['createdAtBlockNumber']\n",
        "query_iter = query_iter.replace('initial',last_block)\n",
        "query = query_iter\n",
        "\n",
        "try:\n",
        "    while(1):\n",
        "        result = run_query(query_iter)\n",
        "        switch_token(result)\n",
        "        print(result.keys())\n",
        "        for pair in result['data']['pairs']:\n",
        "            if((pair['token0']['symbol'] != 'WETH') & (pair['token1']['symbol'] !='WETH' )):\n",
        "              continue\n",
        "            if((pair['token00']['txCount'] == 0) or (pair['token00']['txCount'] == '0')):\n",
        "              continue\n",
        "            year = time.gmtime(int(pair['createdAtTimestamp'])).tm_year\n",
        "            month = time.gmtime(int(pair['createdAtTimestamp'])).tm_mon\n",
        "            day = time.gmtime(int(pair['createdAtTimestamp'])).tm_mday\n",
        "            pair['createdAtTimestamp'] = str(year) + '-' + str(month) + '-' + str(day)\n",
        "            pair_frame.append(pair)\n",
        "        query_iter = query_iter.replace(last_block,result['data']['pairs'][999]['createdAtBlockNumber'])\n",
        "        last_block = result['data']['pairs'][999]['createdAtBlockNumber']\n",
        "        print(last_block)\n",
        "\n",
        "except Exception as e:\n",
        "    try:\n",
        "      print(result['errors'])\n",
        "    except:\n",
        "      print(e)\n",
        "    df = pd.json_normalize(pair_frame)\n",
        "    df.to_csv('Pairs_v1.5.csv',encoding='utf-8-sig',index=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-KO8NdAVtav"
      },
      "source": [
        "#2. Uniswap_API.py 파일\n",
        "\n",
        "input = pairs_v1.x \n",
        "\n",
        "output = mint.json / burn.json / swap.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL1G3hg31G7i"
      },
      "source": [
        "from pprint import pprint\n",
        "from pandas.core.frame import DataFrame\n",
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool\n",
        "from multiprocessing import Process\n",
        "# function to use requests.post to make an API call to the subgraph url\n",
        "\n",
        "global df\n",
        "global df_len\n",
        "df = pd.read_csv('./Pairs_v1.5.csv').to_dict()\n",
        "df_len = len(df['id'])\n",
        "global count\n",
        "count= 0\n",
        "\n",
        "def run_query(query):\n",
        "\n",
        "    # endpoint where you are making the request\n",
        "    request = requests.post('https://api.thegraph.com/subgraphs/name/uniswap/uniswap-v2'\n",
        "                            '',\n",
        "                            json={'query': query})\n",
        "    if request.status_code == 200:\n",
        "        return request.json()\n",
        "    else:\n",
        "        raise Exception('Query failed. return code is {}.      {}'.format(request.status_code, query))\n",
        "\n",
        "\n",
        "mint_query_template = '''\n",
        "{\n",
        "  mints(orderBy: timestamp, orderDirection: asc, where:{ pair: \"pair_address\" }) {\n",
        "      amount0\n",
        "      amount1\n",
        "      to\n",
        "      sender\n",
        "      timestamp\n",
        " }\n",
        "}\n",
        "''' \n",
        "\n",
        "swap_query_template = '''\n",
        "{\n",
        "  swaps(orderBy: timestamp, orderDirection: asc, where:{ pair: \"pair_address\" }) {\n",
        "      amount0In\n",
        "      amount0Out\n",
        "      amount1In\n",
        "      amount1Out\n",
        "      to\n",
        "      sender\n",
        "      timestamp\n",
        " }\n",
        "}\n",
        "''' \n",
        "\n",
        "burn_query_template = '''\n",
        "{\n",
        "  burns(orderBy: timestamp, orderDirection: asc, where:{ pair: \"pair_address\" }) {\n",
        "      amount0\n",
        "      amount1\n",
        "      to\n",
        "      sender\n",
        "      timestamp\n",
        " }\n",
        "}\n",
        "''' \n",
        "\n",
        "#############모든 pair 쌍에 대해서 Mint Query 후 결과 저장##############\n",
        "def get_mint_subProcess(pair_address):\n",
        "    try:\n",
        "        query = mint_query_template.replace('pair_address',pair_address)\n",
        "        result = run_query(query)\n",
        "        return {pair_address : result['data']['mints']}\n",
        "    except:\n",
        "        return {pair_address : ['Error Occur']}\n",
        "        \n",
        "def get_mint():\n",
        "    file_path = \"./mint.json\"\n",
        "    mint_json = {}\n",
        "    try:\n",
        "      p = Pool(8)\n",
        "      start = time.time()\n",
        "      global count\n",
        "      for ret in p.imap(get_mint_subProcess,df['id'].values()):\n",
        "        count = count+1\n",
        "#        print(\"Got value\",ret,\"Time :\",time.time()-start)\n",
        "        mint_json.update(ret) \n",
        "        if(count % 1000 == 0):\n",
        "          print(\"Process Rate : {}/{} {}%\".format(count,df_len,int((count/df_len)*100)))\n",
        "#          print(\"write file count : \" + str(count))\n",
        "#          with open(file_path,'w') as outfile:\n",
        "#            json.dump(mint_json, outfile, indent=4)    \n",
        "  \n",
        "      print('finish ' + str(count))\n",
        "      with open(file_path,'w') as outfile:\n",
        "            json.dump(mint_json, outfile, indent=4)\n",
        "      delta_t = time.time() - start\n",
        "      print(\"Total Time :\",delta_t)\n",
        "      p.close()\n",
        "      p.join()\n",
        "      mint_json.clear()        \n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "#############모든 pair 쌍에 대해서 Mint Query 후 결과 저장##############\n",
        "def get_swap_subProcess(pair_address):\n",
        "    try:\n",
        "        query = swap_query_template.replace('pair_address',pair_address)\n",
        "        result = run_query(query)\n",
        "        return {pair_address : result['data']['swaps']}\n",
        "    except:\n",
        "        return {pair_address : ['Error Occur']}\n",
        "        \n",
        "def get_swap():\n",
        "    file_path = \"./swap.json\"\n",
        "    swap_json = {}\n",
        "    try:\n",
        "        p = Pool(8)\n",
        "        start = time.time()\n",
        "        global count\n",
        "        for ret in p.imap(get_swap_subProcess,df['id'].values()):\n",
        "            count = count+1\n",
        "            swap_json.update(ret)\n",
        "            if(count % 1000 == 0):\n",
        "                print(\"Process Rate : {}/{} {}%\".format(count,df_len,int((count/df_len)*100)))\n",
        " #               print(\"write file count : \" + str(count))\n",
        " #               with open(file_path,'w') as outfile:\n",
        " #                   json.dump(swap_json, outfile, indent=4)    \n",
        "  \n",
        "        p.close()\n",
        "        p.join()\n",
        "        print('finish ' + str(count))\n",
        "        with open(file_path,'w') as outfile:\n",
        "            json.dump(swap_json, outfile, indent=4)\n",
        "        delta_t = time.time() - start\n",
        "        print(\"Total Time :\",delta_t)\n",
        "        swap_json.clear()        \n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "#############모든 pair 쌍에 대해서 Burn Query 후 결과 저장##############\n",
        "def get_burn_subProcess(pair_address):\n",
        "    try:\n",
        "        query = burn_query_template.replace('pair_address',pair_address)\n",
        "        result = run_query(query)\n",
        "        return {pair_address : result['data']['burns']}\n",
        "    except:\n",
        "        return {pair_address : ['Error Occur']}\n",
        "        \n",
        "def get_burn():\n",
        "    file_path = \"./burn.json\"\n",
        "    burn_json = {}\n",
        "    try:\n",
        "        p = Pool(8)\n",
        "        start = time.time()\n",
        "        global count\n",
        "        for ret in p.imap(get_burn_subProcess,df['id'].values()):\n",
        "            count = count+1\n",
        "            burn_json.update(ret)\n",
        "            if(count % 1000 == 0):\n",
        "                print(\"Process Rate : {}/{} {}%\".format(count,df_len,int((count/df_len)*100)))\n",
        "#                print(\"write file count : \" + str(count))\n",
        "#                with open(file_path,'w') as outfile:\n",
        "#                    json.dump(burn_json, outfile, indent=4)    \n",
        "  \n",
        "        p.close()\n",
        "        p.join()\n",
        "        print('finish ' + str(count))\n",
        "        with open(file_path,'w') as outfile:\n",
        "            json.dump(burn_json, outfile, indent=4)\n",
        "        delta_t = time.time() - start\n",
        "        print(\"Total Time :\",delta_t)\n",
        "        burn_json.clear()        \n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "if __name__=='__main__': \n",
        "    get_mint()\n",
        "    get_burn()\n",
        "    get_swap()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tidcOVnWLP7"
      },
      "source": [
        "#3. Initial_Liquidity_Eth.py\n",
        "\n",
        "input = Pairs파일 / mint.json\n",
        "\n",
        "initial_Liquidity ETH / Token 열 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq-k9wld2qkz"
      },
      "source": [
        "from pandas.core.frame import DataFrame\n",
        "from requests import Request, Session\n",
        "import pandas as pd\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import re # 추가\n",
        "from urllib.request import urlopen\n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import Pool\n",
        "import json\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/mint.json', 'r') as f:\n",
        "    mint_json = json.load(f)\n",
        "\n",
        "len(mint_json)\n",
        "datas = pd.read_csv('/content/drive/MyDrive/Pairs_v1.5.csv',encoding='utf-8-sig').to_dict('records')\n",
        "\n",
        "\n",
        "for data in datas:\n",
        "    pair_id = data['id']\n",
        "\n",
        "    if(len(mint_json[pair_id]) == 0 ):     #Mint 정보가 없는 pair는 초기 Liquidity를 0으로 한다.\n",
        "        data['initial_Liquidity_ETH'] = 0\n",
        "        data['initial_Liquidity_token'] = 0\n",
        "        continue\n",
        "\n",
        "\n",
        "    if(data['token0.symbol'] == 'WETH'):\n",
        "        initial_Liquidity_ETH = mint_json[pair_id][0]['amount0']\n",
        "        initial_Liquidity_token = mint_json[pair_id][0]['amount1']\n",
        "    else:\n",
        "        initial_Liquidity_ETH = mint_json[pair_id][0]['amount1']\n",
        "        initial_Liquidity_token = mint_json[pair_id][0]['amount0']\n",
        "\n",
        "    data['initial_Liquidity_ETH'] = initial_Liquidity_ETH  \n",
        "    data['initial_Liquidity_token'] = initial_Liquidity_token\n",
        "\n",
        "\n",
        "\n",
        "DataFrame(datas).to_csv('Pairs_v1.5.csv',encoding='utf-8-sig')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meClmIi1WYkU"
      },
      "source": [
        "#4.Creator.py\n",
        "\n",
        "Creator 긁어 오는건 시간 많이 걸림.\n",
        "이전 버전의 파일에서 Creator를 수동으로 열 추가해주고 없는 부분만 다시 찾아온다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pboau361Gifl",
        "outputId": "80f5c8a3-1395-465e-ab27-690a1c9649c1"
      },
      "source": [
        "#Etherplorer API / Etherscan Crawling으로 Creator Address를 구한다.\n",
        "from pandas.core.frame import DataFrame\n",
        "from requests import Request, Session\n",
        "import pandas as pd\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import re # 추가\n",
        "from urllib.request import urlopen\n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import Pool\n",
        "import os\n",
        "import glob\n",
        " \n",
        "def createFolder(directory):\n",
        "    try:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "    except OSError:\n",
        "        print ('Error: Creating directory. ' +  directory)\n",
        " \n",
        "\n",
        "def split_csv(total_csv):\n",
        "    rows = pd.read_csv(total_csv,chunksize=5000)\n",
        "    file_count = 0\n",
        "    for i, chuck in enumerate(rows):\n",
        "        chuck.to_csv('./result/out{}.csv'.format(i))\n",
        "        file_count = file_count+1 \n",
        "    return file_count\n",
        "\n",
        "def merge_csv():\n",
        "  input_file = r'./result/'\n",
        "  output_file = r'./result/Pairs_v1.5.csv'\n",
        "\n",
        "  allFile_list = glob.glob(os.path.join(input_file, 'fout*')) # glob함수로 sales_로 시작하는 파일들을 모은다\n",
        "  allFile_list.sort()\n",
        "  print(allFile_list)\n",
        "\n",
        "  all_Data = []\n",
        "  for file in allFile_list:\n",
        "    records = pd.read_csv(file).to_dict('records') \n",
        "    all_Data.extend(records)\n",
        "\n",
        "  DataFrame(all_Data).to_csv(output_file,encoding='utf-8-sig',index=False)\n",
        "  \n",
        "def get_creatorAddress(data):\n",
        "    if(str(data['token00_creator_address']) != 'nan'):\n",
        "        return data\n",
        "    token_id = data['token00.id']\n",
        "    repos_url = 'https://api.ethplorer.io/getAddressInfo/'+token_id+'?apiKey=EK-4L18F-Y2jC1b7-9qC3N'\n",
        "    response = requests.get(repos_url).text\n",
        "    repos = json.loads(response)    #json 형태로 token_id에 해당하는 정보를 불러온다.\n",
        "    \n",
        "    try:\n",
        "        creator_address = repos['contractInfo']['creatorAddress']\n",
        "        print('find by ethplorer : ' + token_id)\n",
        "    except:     #오류가 나면 이더스캔에서 크롤링\n",
        "         url = 'https://etherscan.io/address/'+token_id\n",
        "         try:\n",
        "             time.sleep(2)\n",
        "             response = requests.get(url,headers={'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'})\n",
        "             page_soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "             Transfers_info_table_1 = str(page_soup.find(\"a\", {\"class\": \"hash-tag text-truncate\"}))\n",
        "             creator_address = re.sub('<.+?>', '', Transfers_info_table_1, 0).strip()\n",
        "             print('find by etherscan : ' + token_id)\n",
        "             print('result : ' + creator_address)\n",
        "         except Exception as e:  #이더스캔 크롤링까지 에러나면 'Error'로 표시\n",
        "              print(e)\n",
        "              timeout_count = 0\n",
        "              while(timeout_count<2):\n",
        "                  try:\n",
        "                      print('timeout in, address' + token_id)\n",
        "                      time.sleep(20)\n",
        "                      timeout_count = timeout_count + 1\n",
        "                      response = requests.get(repos_url).text\n",
        "                      repos = json.loads(response)\n",
        "                      creator_address = repos['contractInfo']['creatorAddress']\n",
        "                      break\n",
        "                  except:\n",
        "                      creator_address = 'Error'\n",
        "\n",
        "    data['token00_creator_address'] = creator_address\n",
        "    return data\n",
        "\n",
        "if __name__=='__main__':\n",
        "    createFolder('./result')\n",
        "    file_name = 'Pairs_v1.5.csv'\n",
        "    file_count = split_csv(file_name)\n",
        "    out_list = []\n",
        "    out_list = list(input('입력(공백단위) : ').split())\n",
        "\n",
        "    for i in out_list:         #하나의 파일 단위로 Creator Address 불러오고, 해당 초기 유동성풀 이더값 구해온다.\n",
        "        file_name = './result/out{}.csv'.format(i)\n",
        "        datas = pd.read_csv(file_name).to_dict('records')\n",
        "        datas_len = len(datas)\n",
        "        try:\n",
        "            p = Pool(4)\n",
        "            count = 0\n",
        "            result = []\n",
        "            for ret in p.imap(get_creatorAddress,datas):\n",
        "                count = count+1\n",
        "                result.append(ret)\n",
        "                if(count % 200 == 0):\n",
        "                    print(\"Process Rate : {}/{} {}%\".format(count,datas_len,int((count/datas_len)*100)))\n",
        "            p.close()\n",
        "            p.join()\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "        print('=======')\n",
        "        time.sleep(5)\n",
        "            \n",
        "        df = pd.DataFrame(result)\n",
        "        file_name = './result/fout{}.csv'.format(i)\n",
        "        df.to_csv(file_name,encoding='utf-8-sig',index=False)\n",
        "        print(file_name + ' complete')\n",
        "    merge_csv()\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZozgDRanXqbX"
      },
      "source": [
        ""
      ]
    }
  ]
}